Combining fuzz testing with symbolic execution has been proved to be a promising approach to improve the testing performance. Driller, which is built on top of Angr symbolic execution engine and AFL fuzzing engine, has attempted to leverage symbolic execution to assist fuzz testing in exploring as many as paths \cite{stephens2016driller}. And its performance in the CGC challenge \cite{online:CGC} has been the great supporting evidence for this hybrid testing approach. 

We also leveraged this approach to improve the performance of fuzz testing and enhanced it by introducing two improvements. In our method, symbolic loops (loop boundary depends on symbolic data) have been carefully taken care by \textit{Symbolic Loop Bucket (SLB)}. Meanwhile, we introduced a novel method called \textit{Lazy Symbolic Pointer (LSP)} to handle symbolic pointers. In the following sections, the basic workflow of this hybrid testing approach will be introduced briefly first. Then, the details of \textit{SLB} and \textit{LSP} will be discussed.

\subsection{Assist fuzz testing with symbolic execution}
The basic framework of hybrid testing approach based on fuzz testing and symbolic execution is shown in Figure~\ref{s2e-assist}. The fuzzing engine performs evolutionary mutation testing and shares the internal information, which is used to record the path information (such as the \textit{Bitmap} in AFL), with the symbolic execution engine. 
When the fuzzing engine starts to be saturated, the current seed file will be sent to symbolic execution engine to uncover more new paths. 
Unlike traditional unlimited symbolic execution, the engine in hybrid testing method tries to limit the number of states forked into an acceptable scale. For example, as shown in Figure~\ref{s2e-assist}, suppose the red execution trace is the path which the seed file will taken, the symbolic execution engine only forks when it finds an uncovered branch (the yellow branch) according to the shared internal information from fuzzing engine. And these forked state from the yellow branches will be prohibited from forking any new states once the total number of states reaches the budget. And Each forked state will generate a new test case which can help fuzzing engine to touch more new code areas. 

Compared with using fuzz testing and symbolic execution separately, this hybrid approach can take the advantages of both methods and mitigate the ``path explosion'' problem by this scale controllable concolic execution. Meanwhile, the fuzzing engine will be benefit from the \textit{fresh} test cases generated by symbolic execution.

\begin{figure}
\centering
\includegraphics[width=0.7\textwidth]{figures/s2e-assist.pdf} 
\caption{Assist fuzz testing with concolic execution.}\label{s2e-assist}
\end{figure}


\subsection{Symbolic Loop Buckets}
Symbolic loop, whose loop control variable depends on symbolic data, will cause serious ``path explosion'' problem as its loop times will range from 0 to infinite for the worst case. 
Even though the hybrid testing method can ease the ``path explosion'' problem, but the states forked from the symbolic loop will quickly force the number of states increasing to the budget. So the symbolic loops have to be carefully handled to avoid getting stuck at the loop spot. AFL utilizes \emph{Loop Bucket} mechanism to avoid collecting too many test cases which only affect the loop times into the seed queue. It groups the loop times into 8 different buckets, i.e. [1, 2, 3, 4-7, 8-15, 16-31, 32-127, 128+]. Only changes happened between different buckets will be regarded as new behaviors. Based on this idea, we proposed a \textit{Symbolic Loop Bucket (SLB)} to handle the symbolic loop when performing hybrid testing. The algorithm of SLB is described in Algorithm~\ref{SLB}.

Loops are extracted from the target program by static analysis. These loops will be configured to symbolic execution engine to help it recognize loops in runtime. All the symbolic loops can be distinguished from the others by checking whether the loop exit condition is affected by symbolic data or not (Line 1$\sim$3). For the edge belong to symbolic loop, the uncovered loop buckets for this loop will be obtained by analyzing the \textit{Bitmap} mentioned before (Line 5). 
State forking is disabled in already covered loop buckets, and the program will loop for one more time (Line 16\&17). Once an uncovered bucket is reached, the corresponding test case will be generated and then this uncovered bucket will be removed from the uncovered loop buckets to avoid generating multiple test cases (Line 7$\sim$12). After generating test cases for all the uncovered buckets, the loop will be prohibited from being executed for more times. This can make sure that all the loop buckets will be covered without coming across with the ``path explosion'' problem.

\begin{algorithm}
  \caption{Symbolic Loop Bucket}
  \label{SLB}
  \KwIn{Configured Loops $L$, Current Edge $CE$ and Bitmap $B_p$}
  \KwOut{Generated test cases $t_{slb}$}  
  \If{not $IsaLoopCycleEdge(L,CE)$ or not $IsaSymLoop(CE)$}
  {
    return;
  }
  $loopTimes$ = 1\;
  $UBs$ = $ParseUncoveredBuckets(B_p)$\;
  \While{TRUE}
  {
    \ForEach{$ub$ in $UBs$}
    {
      \If{$loopTimes$ within $ub$}
      {
        $t_{slb}.add(GenerateTestcase())$\;
        $UBs$.$remove(ub)$\;
      }
    }
    \eIf{$UBs$ is $null$}
    {
      return $t_{lsb}$\;
    }{
      $ExecuteOneCycle()$\;
      $loopTimes += 1$\;
    }
  }
\end{algorithm}  

\subsection{Lazy Symbolic Pointer}
The motivated code snippet in Listing~\ref{RE-LSP} shows the basic symbolic pointer problem in symbolic execution. The \texttt{EAX} register in line 6 is a symbolic data that derived from the input file. And text\_char is a concrete const Boolean Array that represents the whether the \texttt{EAX} value is an ASCII text. Then based on the returned value from text\_char, the code will perform some further process (Line 8$\sim$10). Because the \texttt{EAX} in Line 6 is marked as symbolic, so it will lead to the symbolic pointer problem as it can point to multiple addresses.


When given a symbolic pointer, based on different memory models, different handle process can be taken. For example, in full symbolic memory model, each possible value of this pointer will fork a corresponding state. There also exists a strategy namely address concretization which will concretize the pointer to a single specific address. Obviously, the full symbolic memory model will raise the serious state explosion and the address concretization may lose some interesting paths. To mitigate the scalability problems of full symbolic memory model and the loss of interesting paths of address concretization, a partial symbolic memory model has been to make a trade-off in recent years. The partial symbolic memory model tries to concretize all symbolic pointer write operation and treats all symbolic pointer read operation as full symbolic memory model. 

The lazy forking strategy in S2E is also leveraged to ease the problem along with the large number of states forked from symbolic pointer read operation in partial symbolic memory model. Lazy forking is utilized to avoid maintaining expensive symbolic pointers in concolic execution. For example, for the instruction in Line 6 in Listing~\ref{RE-LSP}, suppose \texttt{EAX} is read from the input file and its concrete value is 0x41. Then when the concolic execution engine executes to this statement, it will try to fork the current state with a hard constraint which can be expressed as ``$EAX == 0x41$''. Unlike the other memory models, the engine will not fork all the possible value for \texttt{EAX} but only add the soft constraint of ``$EAX \neq 0x41$'' to the other forked state, which will reduce the number of state to a large extent. 

While because of the hard constraint, the state forking at Line 10 will fail for the original state which means some soundness paths will not be covered by this state. As the objective of concolic execution is to assist fuzz testing to uncover more paths as soon as possible, so this will bring some performance loss. To mitigate this problem, we introduced a method called Lazy Symbolic Pointer which is built on top of partial symbolic memory model and lazy forking. The detailed algorithm of LSP is shown in Algorithm~\ref{LSP}.
\lstinputlisting[label={RE-LSP}, language=nasm,style=nasm,caption={A real-world binary code that contains symbolic pointer dereference.}]{codes/real-eaxmple-LSP.asm}

\begin{algorithm}
  \caption{Lazy Symbolic Pointer}
  \label{LSP}
  \KwIn{Current State $S$, Pending States $S_P$ and Hard Constraint $C_H$}
  \KwOut{Testcase $t_{lsp}$ if success}
  $C_F = getFailedCondition()$\;
  $offs = S.getInputOffset(C_F)$\;
  \If{$S_P.find(offs) == S_P.end()$}
  {
    return $null$\;
  }
  $Conditions = S.getRelateConditions(offs).strip(C_H)$\;
  \ForEach{$s_p$ in $S_P.find(offs)$}
  {
    $s_{tmp} = s_p.clone()$\;
    $s_{tmp}.addConstraint(Conditions)$\;
    $s_{tmp}.addConstraint(C_F)$\;
    $(success, t_lsp) = s_{tmp}.generateTestcase()$\;
    \eIf{$success$}
    {
      return $t_{lsp}$;
    } {
      continue;
    }
  }
  return $null$\;
\end{algorithm}
  
When performing lazy forking, all the states whose path constraints contain the soft constraints will be collected into the \emph{Pending States}. And the pending states are grouped by the input offsets that affect the corresponding soft constraint (Line 6\&7). For the soft constraint that may be affected by more than one input offsets, the pending state will be collected to the groups of all the offsets in the constraint. For example, as the soft constraint ``Not (Eq (ReadLSB w32 0x10 symbolic\_input, 0x41))'' is affected by four bytes (i.e. 0x10, 0x11, 0x12 and 0x13), so the pending state will be appended to the groups indexed by 0x10, 0x11, 0x12 and 0x13. 

Then when the symbolic execution engine fails to fork at the branch that depends on soft constraints, such as the branch at line 10 in Listing~\ref{RE-LSP}, the failed condition will be inspected to extract the input offsets that affect this branch (Line 10\&11). LSP ignores the case that the reason of fork failure does not result from the lazy forking (Line 12\&13). For every offset that affects the failed condition, we extract all the related conditions in path constraints except for the hard constraint (Line 14), and then these conditions will be added to the pending states that related to the input offset to generate test cases (Line 15$\sim$18). 

% TODO: Add a real example, or it is hard to understand %

Based on this algorithm, we can generate at least one new test case that satisfies the failed condition where the fork failure occurs because of lazy forking. But we still need to prove this test case will steer the program to execute the same path with the original state and then covers the branch that fails in the original state. A quick execution consistency proven is explained as follows:

Let $P_A$ is the execution path of a state $A$:
\begin{center}
$P_A = (B_0,T) \rightarrow (B_1,T) \rightarrow (B_2,F) \rightarrow (B_3,T)
\rightarrow (B_4,F) \rightarrow (B_5,F) \rightarrow (B_6,F) \rightarrow (B_u,T)$
\end{center}

\noindent where $B_n$ refers to the $n-th$ branch. $T/F$ means state $A$ takes the \emph{True/False} branch and $B_u$ denotes the branch where the forking fails because of the extra hard constraint. Assume that hard constraint is $var==0xAB$ which is originated from the lazy forking from $B_1$, and the conditions at $B_3$ and $B_5$ are also affected by $var$.

Suppose state $B$ is the corresponding pending state forked from $B_1$, and then, according to the LSP algorithm in Listing~\ref{LSP}, when state $A$ raises a fork failure at branch $B_u$, all the related constraints except the hard constraint will be added to state $B$. After that, the path constraint of state $B$ will be:
\begin{center}
$PC_B = \displaystyle ^\neg CB_u \cap ^\neg CB_1 \cap \bigcap\limits_{i=0,i \neq 1}^{6} CB_i$
\end{center}
\noindent where the $CB_i$ denotes the path condition at branch $B_i$. We need to prove the following formula.
\begin{center}
$\mathbb{Z} = \{var\arrowvert PC_B(var) = True\} \neq \emptyset$
\end{center}

As there are only two branches before $B_u$ that depend on $var$, so the expression $PC_B$ can be simplified to:
\begin{center}
$PC_B = ^\neg CB_u \cap ^\neg CB_1 \cap CB_3 \cap CB_5$
\end{center}

There are two possible cases for $CB_3 \cap CB_5$:
\begin{center}
case1: $CB_3 \cap CB_5 = (var == 0xAB)$

case2: $CB_3 \cap CB_5 \neq (var == 0xAB)$
\end{center}

Under the first case, the path to $B_u$ is only feasible when $var$ equals to 0xAB. So the edge $(B_u, F)$ will never be satisfied (i.e. \emph{dead code}) because $(var == 0xAB) \subseteq CB_u$. For the second case, there must be at least one feasible solution for $var$ that satisfies with the soft constraint $^\neg CB_1$, so $^\neg CB_1 \cap \bigcap_{i=0,i \neq 1}^{6} CB_i$ can be evaluated to $True$ for some specified values of $var$. This has proved that there must be at least one test case under $PC_B$ that can steer the program to the forking failure point $B_u$.

Similarly, for branch condition $B_u$, there are also two possible cases:
\begin{center}
case1: $\displaystyle ^\neg CB_1 \cap \bigcap\limits_{i=0,i\neq 1}^6 CB_i \subseteq CB_u$

case2: $\displaystyle ^\neg CB_1 \cap \bigcap\limits_{i=0,i\neq 1}^6 CB_i \nsubseteq CB_u$
\end{center}

The first case, which can also be expressed as $^\neg CB_u \cap ^\neg CB_1 \cap \bigcap_{i=0,i\neq 1}^6 CB_i = \emptyset$, is the case of \emph{dead code}. And the second case can be transformed into:
\begin{center}
$\displaystyle ^\neg CB_u \cap ^\neg CB_1 \cap \bigcap\limits_{i=0,i\neq 1}^6 CB_i \neq \emptyset \implies \mathbb{Z} = \{var\arrowvert PC_B(var) = True\} \neq \emptyset$
\end{center}

\noindent which means there must be at least one feasible value that satisfies with the false branch of $B_u$. So above all, we can draw a conclusion that our \emph{LSP} algorithm can keep the execution consistency when $(B_u, F)$ is not dead code.